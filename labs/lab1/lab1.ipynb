{"cells":[{"cell_type":"markdown","source":["# Azure Discovery Day 2019\n## Analytics with NRT Intelligence on Azure\n## Ingest, Transform, Emit\n\n#### Summary\nIn this Python Jupyter notebook, you will:\n1. Connect to Azure storage\n2. Ingest data from CSV files in Azure storage to Spark dataframes\n3. Conform and merge heterogenous data sets using the Spark dataframe API\n4. Emit data to Azure storage in Parquet file format\n\nAdditionally, there are optional steps to create Hive tables on the data, query them with Spark SQL, as well as some exploratory data analysis (EDA)."],"metadata":{}},{"cell_type":"code","source":["## Need some library includes\n\nimport os\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import broadcast, lit"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Variables"],"metadata":{}},{"cell_type":"code","source":["# Define some variables to minimize \"hard-coding\" in below cells. Note that variables could also be defined in a separate notebook for greater share-ability.\n\n# Azure storage account information. Note that in production, we would store sensitive info in an appropriate secret store,\n# such as a Databricks secret store backed by an Azure Key Vault. For a hackathon/workshop, we'll put them here for simplicity.\nstorage_acct_name = \"PROVIDE\"\nstorage_acct_key = \"PROVIDE\"\ncontainer_name = \"data\"\n\n# The mount point in the DBFS file system - this will look like a local folder but points to the Azure storage location\nmount_point = \"/mnt/\" + container_name + \"/\"\n\n# Reference data files path root\npath_root_ref = mount_point + \"reference-data/\"\n\n# Transactional data files path root\npath_root_data = mount_point + \"transactional-data-small/\"\n\n# Parquet data files output root\npath_root_parquet = mount_point + \"parquet/\"\n\n# Parquet ref data files output root\npath_root_parquet_ref = path_root_parquet + \"reference/\"\n\n# Parquet trip data files output root\npath_root_parquet_trips_yellow = path_root_parquet + \"trips-yellow/\"\npath_root_parquet_trips_green = path_root_parquet + \"trips-green/\"\npath_root_parquet_trips_all = path_root_parquet + \"trips-all/\"\n\n# Set number of Parquet files to output. Simply hard-code here, could also calculate based on ... # of worker nodes/cores etc.\nnum_of_parquet_files = 8"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Functions"],"metadata":{}},{"cell_type":"code","source":["## Function to get a Spark DataFrame from a CSV source file\n\ndef GetDataFrameFromCsvFile(schema, path_src_file, delimiter):\n  df = spark\\\n    .read\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"delimiter\", delimiter)\\\n    .schema(schema)\\\n    .load(path_src_file)\n  \n  return df;"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["## Given a reference dataframe (this would not make sense for large transaction dataframes), broadcast it across the cluster, lazy-cache it, and return the count, which instantiates the dataframe\n\ndef HandleReferenceDataFrame(df):\n  broadcast(df)\n  df.cache()\n  count = df.count()\n  \n  return count;"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["## Delete Spark job residual files (_SUCCESS, _start*, _committed*) down the folder/file hierarchy\n\ndef CleanupSparkJobFiles(path_root):\n  file_paths = GetFilesRecursive(path_root)\n  \n  for file_path in file_paths:\n    # Get just the file name\n    file_name = os.path.basename(file_path)\n    # print(file_name)\n    \n    if file_name.startswith(\"_\"):\n      # Temp job file - delete it\n      dbutils.fs.rm(file_path)\n    # elif file_name.endswith(\".parquet\"):\n      # Data file - no op\n    # else:\n      # Something else - no op"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["## Get iterable file list. Flattens hierarchical folder/file structure.\n\ndef GetFilesRecursive(path_root):\n  final_list = []\n\n  for directoryItem in dbutils.fs.ls(path_root):\n    directoryItemPathClean = directoryItem.path.replace(\"%25\", \"%\").replace(\"%25\", \"%\")\n    \n    if directoryItem.isDir() == True:\n      final_list = final_list + GetFilesRecursive(directoryItemPathClean)\n    else:\n      final_list.append(directoryItemPathClean)\n  \n  return final_list;"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Connect to Azure Storage"],"metadata":{}},{"cell_type":"code","source":["## Use the Databricks file system utilities to mount a Databricks file system location (/mnt/YOUR CONTAINER NAME) that points to the Azure storage account where data files are located\n## We use variables defined above and string concatenation here so that no \"hard-coding\" is needed\n\ndbutils.fs.mount(\n  source = \"wasbs://\" + container_name + \"@\" + storage_acct_name + \".blob.core.windows.net\",\n  mount_point = mount_point,\n  extra_configs = {\"fs.azure.account.key.\" + storage_acct_name + \".blob.core.windows.net\":storage_acct_key}\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["## This is included to remove the Azure storage mount\n## Commented out since not needed for the lab, but included here \"just in case\" for debugging/experimenting - for example, mount, unmount, try something different, mount again\n\n# dbutils.fs.unmount(mount_point)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["## List contents of the Azure storage account to validate successful connect and mount\n## We are using the Databricks display() function here to improve the esthetics of the output\n\ndisplay(dbutils.fs.ls(mount_point))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Cleanup - delete Parquet output folder if it's present\n# NOTE!! This will DELETE any previous Parquet that you or others have emitted to this storage location!\n\ndbutils.fs.rm(path_root_parquet, True)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Load Reference Data Files into DataFrames"],"metadata":{}},{"cell_type":"markdown","source":["##### Define variables to hold the source path for each of the reference data files"],"metadata":{}},{"cell_type":"code","source":["src_file_ref_payment_type = path_root_ref + \"payment_type_lookup.csv\"\nsrc_file_ref_rate_code = path_root_ref + \"rate_code_lookup.csv\"\nsrc_file_ref_taxi_zone = path_root_ref + \"taxi_zone_lookup.csv\"\nsrc_file_ref_trip_month = path_root_ref + \"trip_month_lookup.csv\"\nsrc_file_ref_trip_type = path_root_ref + \"trip_type_lookup.csv\"\nsrc_file_ref_vendor = path_root_ref + \"vendor_lookup.csv\""],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["##### Define explicit schemas for each of the reference data files\n\nWe could also ingest files with schema inference (i.e. tell Spark to try to figure it out) but let's be explicit here for greater control."],"metadata":{}},{"cell_type":"code","source":["## Payment type\nschema_ref_payment_type = StructType([\n    StructField(\"payment_type\", IntegerType(), True),\n    StructField(\"abbreviation\", StringType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n## Rate code ID\nschema_ref_rate_code = StructType([\n    StructField(\"rate_code_id\", IntegerType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n## Taxi zone\nschema_ref_taxi_zone = StructType([\n    StructField(\"location_id\", StringType(), True),\n    StructField(\"borough\", StringType(), True),\n    StructField(\"zone\", StringType(), True),\n    StructField(\"service_zone\", StringType(), True)\n])\n\n## Trip month\nschema_ref_trip_month = StructType([\n    StructField(\"trip_month\", StringType(), True),\n    StructField(\"month_name_short\", StringType(), True),\n    StructField(\"month_name_full\", StringType(), True)\n])\n\n## Trip type\nschema_ref_trip_type = StructType([\n    StructField(\"trip_type\", IntegerType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n## Vendor ID\nschema_ref_vendor = StructType([\n    StructField(\"vendor_id\", IntegerType(), True),\n    StructField(\"abbreviation\", StringType(), True),\n    StructField(\"description\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["##### Load each reference data set into a Spark DataFrame\n\nWe load the data from source file into dataframe using a function (above) for that purpose.\n\nThen we do some more optimizations for the reference dataframes:\n1. Broadcast the dataframe. These are small dataframes with reference data. Broadcasting means we replicate a dataframe to each worker node in a Spark cluster, so that cross-node (cross-network) joins are avoided.\n2. Lazy-cache the dataframe into memory as another performance optimization.\n\nLast, we print the number rows in the dataframe."],"metadata":{}},{"cell_type":"code","source":["df_ref_payment_type = GetDataFrameFromCsvFile(schema_ref_payment_type, src_file_ref_payment_type, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_payment_type))\ndisplay(df_ref_payment_type)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["df_ref_rate_code = GetDataFrameFromCsvFile(schema_ref_rate_code, src_file_ref_rate_code, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_rate_code))\ndisplay(df_ref_rate_code)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["df_ref_taxi_zone = GetDataFrameFromCsvFile(schema_ref_taxi_zone, src_file_ref_taxi_zone, \",\")\n\nprint(HandleReferenceDataFrame(df_ref_taxi_zone))\ndisplay(df_ref_taxi_zone)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["df_ref_trip_month = GetDataFrameFromCsvFile(schema_ref_trip_month, src_file_ref_trip_month, \",\")\n\nprint(HandleReferenceDataFrame(df_ref_trip_month))\ndisplay(df_ref_trip_month)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["df_ref_trip_type = GetDataFrameFromCsvFile(schema_ref_trip_type, src_file_ref_trip_type, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_trip_type))\ndisplay(df_ref_trip_type)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["df_ref_vendor = GetDataFrameFromCsvFile(schema_ref_vendor, src_file_ref_vendor, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_vendor))\ndisplay(df_ref_vendor)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### Write reference data out to Parquet files\n\nParquet files are faster to load than CSV. They also support partitioning, but for the small reference data files, we coalesce the dataframe to 1 piece and we do not partition."],"metadata":{}},{"cell_type":"code","source":["df_ref_payment_type.coalesce(1).write.parquet(path_root_parquet_ref + \"payment-type/\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["df_ref_rate_code.coalesce(1).write.parquet(path_root_parquet_ref + \"rate-code/\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["df_ref_taxi_zone.coalesce(1).write.parquet(path_root_parquet_ref + \"taxi-zone/\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["df_ref_trip_month.coalesce(1).write.parquet(path_root_parquet_ref + \"trip-month/\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["df_ref_trip_type.coalesce(1).write.parquet(path_root_parquet_ref + \"trip-type/\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["df_ref_vendor.coalesce(1).write.parquet(path_root_parquet_ref + \"vendor/\")"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["### Load transactions (trip data) into DataFrames"],"metadata":{}},{"cell_type":"code","source":["## Canonical ordered column list to homogenize schema - we will conform all ingested data sets to this schema\n\ncolumns_rides_canonical = [\n    \"trip_type\",\n\t\"trip_year\",\n\t\"trip_month\",\n\t\"taxi_type\",\n\t\"vendor_id\",\n\t\"pickup_datetime\",\n\t\"dropoff_datetime\",\n\t\"passenger_count\",\n\t\"trip_distance\",\n\t\"rate_code_id\",\n\t\"store_and_fwd_flag\",\n\t\"pickup_location_id\",\n\t\"dropoff_location_id\",\n\t\"pickup_longitude\",\n\t\"pickup_latitude\",\n\t\"dropoff_longitude\",\n\t\"dropoff_latitude\",\n\t\"payment_type\",\n\t\"fare_amount\",\n\t\"extra\",\n\t\"mta_tax\",\n\t\"tip_amount\",\n\t\"tolls_amount\",\n\t\"improvement_surcharge\",\n    \"ehail_fee\",\n\t\"total_amount\"\n]"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Define canonical schema so that we can create an empty DataFrame with the canonical schema into which we will merge all the invididual data file-ingested DataFrames\n\nschema_rides_canonical = StructType([\n    StructField(\"trip_type\", IntegerType(), True),\n    StructField(\"trip_year\", IntegerType(), True),\n    StructField(\"trip_month\", IntegerType(), True),\n    StructField(\"taxi_type\", StringType(), True),\n    StructField(\"vendor_id\", StringType(), True),\n    StructField(\"pickup_datetime\", TimestampType(), True),\n    StructField(\"dropoff_datetime\", TimestampType(), True),\n    StructField(\"passenger_count\", IntegerType(), True),\n    StructField(\"trip_distance\", DoubleType(), True),\n    StructField(\"rate_code_id\", IntegerType(), True),\n    StructField(\"store_and_fwd_flag\", StringType(), True),\n    StructField(\"pickup_location_id\", IntegerType(), True),\n    StructField(\"dropoff_location_id\", IntegerType(), True),\n    StructField(\"pickup_longitude\", StringType(), True),\n    StructField(\"pickup_latitude\", StringType(), True),\n    StructField(\"dropoff_longitude\", StringType(), True),\n    StructField(\"dropoff_latitude\", StringType(), True),\n    StructField(\"payment_type\", StringType(), True),\n    StructField(\"fare_amount\", DoubleType(), True),\n    StructField(\"extra\", DoubleType(), True),\n    StructField(\"mta_tax\", DoubleType(), True),\n    StructField(\"tip_amount\", DoubleType(), True),\n    StructField(\"tolls_amount\", DoubleType(), True),\n    StructField(\"improvement_surcharge\", DoubleType(), True),\n    StructField(\"ehail_fee\", DoubleType(), True),\n    StructField(\"total_amount\", DoubleType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["#### Define a DataFrame to hold ALL rides - all years/months, both Yellow and Green"],"metadata":{}},{"cell_type":"code","source":["df_all_rides_canonical = spark.createDataFrame([], schema_rides_canonical)\ndf_all_rides_canonical.cache()\n\ndf_all_rides_canonical.printSchema()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["#### Load taxi rides - Yellow"],"metadata":{}},{"cell_type":"markdown","source":["##### Define source file schemas - Yellow\n\nThese vary by year. We have to define several schemas to fit the different source file layouts."],"metadata":{}},{"cell_type":"code","source":["## 2016H2, 2017, 2018\nschema_rides_yellow_16H2to18 = StructType([\n    StructField(\"VendorID\", StringType(), True),\n    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n    StructField(\"passenger_count\", IntegerType(), True),\n    StructField(\"trip_distance\", DoubleType(), True),\n    StructField(\"RatecodeID\", IntegerType(), True),\n    StructField(\"store_and_fwd_flag\", StringType(), True),\n    StructField(\"PULocationID\", IntegerType(), True),\n    StructField(\"DOLocationID\", IntegerType(), True),\n    StructField(\"payment_type\", StringType(), True),\n    StructField(\"fare_amount\", DoubleType(), True),\n    StructField(\"extra\", DoubleType(), True),\n    StructField(\"mta_tax\", DoubleType(), True),\n    StructField(\"tip_amount\", DoubleType(), True),\n    StructField(\"tolls_amount\", DoubleType(), True),\n    StructField(\"improvement_surcharge\", DoubleType(), True),\n    StructField(\"total_amount\", DoubleType(), True)\n])\n\n## 2015 and 2016H1\nschema_rides_yellow_15to16H1 = StructType([\n    StructField(\"VendorID\", StringType(), True),\n    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n    StructField(\"passenger_count\", IntegerType(), True),\n    StructField(\"trip_distance\", DoubleType(), True),\n    StructField(\"pickup_longitude\", DoubleType(), True),\n    StructField(\"pickup_latitude\", DoubleType(), True),\n    StructField(\"RatecodeID\", IntegerType(), True),\n    StructField(\"store_and_fwd_flag\", StringType(), True),\n    StructField(\"dropoff_longitude\", DoubleType(), True),\n    StructField(\"dropoff_latitude\", DoubleType(), True),\n    StructField(\"payment_type\", StringType(), True),\n    StructField(\"fare_amount\", DoubleType(), True),\n    StructField(\"extra\", DoubleType(), True),\n    StructField(\"mta_tax\", DoubleType(), True),\n    StructField(\"tip_amount\", DoubleType(), True),\n    StructField(\"tolls_amount\", DoubleType(), True),\n    StructField(\"improvement_surcharge\", DoubleType(), True),\n    StructField(\"total_amount\", DoubleType(), True)\n])\n\n## 2010 though 2014\nschema_rides_yellow_10to14 = StructType([\n    StructField(\"vendor_id\", StringType(), True),\n    StructField(\"pickup_datetime\", TimestampType(), True),\n    StructField(\"dropoff_datetime\", TimestampType(), True),\n    StructField(\"passenger_count\", IntegerType(), True),\n    StructField(\"trip_distance\", DoubleType(), True),\n    StructField(\"pickup_longitude\", DoubleType(), True),\n    StructField(\"pickup_latitude\", DoubleType(), True),\n    StructField(\"rate_code\", IntegerType(), True),\n    StructField(\"store_and_fwd_flag\", StringType(), True),\n    StructField(\"dropoff_longitude\", DoubleType(), True),\n    StructField(\"dropoff_latitude\", DoubleType(), True),\n    StructField(\"payment_type\", StringType(), True),\n    StructField(\"fare_amount\", DoubleType(), True),\n    StructField(\"surcharge\", DoubleType(), True),\n    StructField(\"mta_tax\", DoubleType(), True),\n    StructField(\"tip_amount\", DoubleType(), True),\n    StructField(\"tolls_amount\", DoubleType(), True),\n    StructField(\"total_amount\", DoubleType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["##### Schema helper functions - Yellow"],"metadata":{}},{"cell_type":"code","source":["# Function to add columns to dataframe as required to homogenize schema\n# Input:  Dataframe, year and month\n# Output: Dataframe with homogenized schema \n\ndef GetSchemaHomogenizedDataframe_Yellow(source_df, trip_year, trip_month):\n  years10To14 = [2010, 2011, 2012, 2013, 2014]\n  \n  if (trip_year >= 2017 or (trip_year == 2016 and trip_month > 6)):\n    df = source_df\\\n      .withColumn(\"trip_type\", lit(0))\\\n      .withColumn(\"trip_year\", source_df.tpep_pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.tpep_pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"yellow\"))\\\n      .withColumn(\"temp_vendor_id\", source_df.VendorID.cast(StringType()))\\\n      .drop(\"VendorID\")\\\n      .withColumnRenamed(\"temp_vendor_id\", \"vendor_id\")\\\n      .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\\\n      .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\\\n      .withColumnRenamed(\"RatecodeID\", \"rate_code_id\")\\\n      .withColumnRenamed(\"PULocationID\", \"pickup_location_id\")\\\n      .withColumnRenamed(\"DOLocationID\", \"dropoff_location_id\")\\\n      .withColumn(\"pickup_longitude\", lit(\"\"))\\\n      .withColumn(\"pickup_latitude\", lit(\"\"))\\\n      .withColumn(\"dropoff_longitude\", lit(\"\"))\\\n      .withColumn(\"dropoff_latitude\", lit(\"\"))\\\n      .withColumn(\"temp_payment_type\", source_df.payment_type.cast(StringType()))\\\n      .drop(\"payment_type\")\\\n      .withColumnRenamed(\"temp_payment_type\", \"payment_type\")\\\n      .withColumn(\"ehail_fee\", lit(0.0))\n\n      # passenger_count\n      # trip_distance\n      # store_and_fwd_flag\n      # fare_amount\n      # extra\n      # mta_tax\n      # tip_amount\n      # tolls_amount\n      # improvement_surcharge\n      # total_amount\n  elif ((trip_year == 2016 and trip_month <= 6) or (trip_year == 2015)):\n    df = source_df\\\n      .withColumn(\"trip_type\", lit(0))\\\n      .withColumn(\"trip_year\", source_df.tpep_pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.tpep_pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"yellow\"))\\\n      .withColumn(\"temp_vendor_id\", source_df.VendorID.cast(StringType()))\\\n      .drop(\"VendorID\")\\\n      .withColumnRenamed(\"temp_vendor_id\", \"vendor_id\")\\\n      .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\\\n      .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\\\n      .withColumnRenamed(\"RatecodeID\", \"rate_code_id\")\\\n      .withColumn(\"pickup_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"dropoff_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"temp_pickup_longitude\", source_df.pickup_longitude.cast(StringType()))\\\n      .drop(\"pickup_longitude\")\\\n      .withColumnRenamed(\"temp_pickup_longitude\", \"pickup_longitude\")\\\n      .withColumn(\"temp_pickup_latitude\", source_df.pickup_latitude.cast(StringType()))\\\n      .drop(\"pickup_latitude\")\\\n      .withColumnRenamed(\"temp_pickup_latitude\", \"pickup_latitude\")\\\n      .withColumn(\"temp_dropoff_longitude\", source_df.dropoff_longitude.cast(StringType()))\\\n      .drop(\"dropoff_longitude\")\\\n      .withColumnRenamed(\"temp_dropoff_longitude\", \"dropoff_longitude\")\\\n      .withColumn(\"temp_dropoff_latitude\", source_df.dropoff_latitude.cast(StringType()))\\\n      .drop(\"dropoff_latitude\")\\\n      .withColumnRenamed(\"temp_dropoff_latitude\", \"dropoff_latitude\")\\\n      .withColumn(\"temp_payment_type\", source_df.payment_type.cast(StringType()))\\\n      .drop(\"payment_type\")\\\n      .withColumnRenamed(\"temp_payment_type\", \"payment_type\")\\\n      .withColumn(\"ehail_fee\", lit(0.0))\n\n      # passenger_count\n      # trip_distance\n      # store_and_fwd_flag\n      # fare_amount\n      # extra\n      # mta_tax\n      # tip_amount\n      # tolls_amount\n      # improvement_surcharge\n      # total_amount\n  elif (trip_year in years10To14):\n    df = source_df\\\n      .withColumn(\"trip_type\", lit(0))\\\n      .withColumn(\"trip_year\", source_df.pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"yellow\"))\\\n      .withColumn(\"temp_vendor_id\", source_df.vendor_id.cast(StringType()))\\\n      .drop(\"vendor_id\")\\\n      .withColumnRenamed(\"temp_vendor_id\", \"vendor_id\")\\\n      .withColumnRenamed(\"rate_code\", \"rate_code_id\")\\\n      .withColumn(\"pickup_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"dropoff_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"temp_pickup_longitude\", source_df.pickup_longitude.cast(StringType()))\\\n      .drop(\"pickup_longitude\")\\\n      .withColumnRenamed(\"temp_pickup_longitude\", \"pickup_longitude\")\\\n      .withColumn(\"temp_pickup_latitude\", source_df.pickup_latitude.cast(StringType()))\\\n      .drop(\"pickup_latitude\")\\\n      .withColumnRenamed(\"temp_pickup_latitude\", \"pickup_latitude\")\\\n      .withColumn(\"temp_dropoff_longitude\", source_df.dropoff_longitude.cast(StringType()))\\\n      .drop(\"dropoff_longitude\")\\\n      .withColumnRenamed(\"temp_dropoff_longitude\", \"dropoff_longitude\")\\\n      .withColumn(\"temp_dropoff_latitude\", source_df.dropoff_latitude.cast(StringType()))\\\n      .drop(\"dropoff_latitude\")\\\n      .withColumnRenamed(\"temp_dropoff_latitude\", \"dropoff_latitude\")\\\n      .withColumn(\"temp_payment_type\", source_df.payment_type.cast(StringType()))\\\n      .drop(\"payment_type\")\\\n      .withColumnRenamed(\"temp_payment_type\", \"payment_type\")\\\n      .withColumnRenamed(\"surcharge\", \"extra\")\\\n      .withColumn(\"improvement_surcharge\",lit(0).cast(DoubleType()))\\\n      .withColumn(\"ehail_fee\", lit(0.0))\n\n      # pickup_datetime\n      # dropoff_datetime\n      # passenger_count\n      # trip_distance \n      # store_and_fwd_flag\n      # fare_amount\n      # mta_tax\n      # tip_amount\n      # tolls_amount\n      # total_amount\n\n  return df;"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# Function to return schema for a given year and month\n# Input:  Year and month\n# Output: StructType for applicable schema \n\ndef GetTaxiSchema_Yellow(trip_year, trip_month):\n  years10To14 = [2010, 2011, 2012, 2013, 2014]\n  \n  if (trip_year >= 2017 or (trip_year == 2016 and trip_month > 6)):\n    schema = schema_rides_yellow_16H2to18\n  elif ((trip_year == 2016 and trip_month <= 6) or trip_year == 2015):\n    schema = schema_rides_yellow_15to16H1\n  elif (trip_year in years10To14):\n    schema = schema_rides_yellow_10to14\n\n  return schema;"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["##### Load data files\n\n1. Load each year/month data file to a dataframe\n2. Persist to Parquet for that year/month\n3. Append to the \"all rides\" DataFrame"],"metadata":{}},{"cell_type":"code","source":["for yyyy in range(2010, 2019):\n  start_month = 1\n  end_month = 12\n  \n  # The dataset goes up to June 2018 inclusive (at the time of this writing - if the dataset is expanded with all of 2018, this should be changed)\n  if yyyy == 2018:\n    end_month = 6\n\n  #print(\"yyyy=\" + str(yyyy))\n  #print(\"start_month=\" + str(start_month))\n  #print(\"end_month=\" + str(end_month))\n\n  yyyys = str(yyyy)\n  #print(\"yyyys=\" + yyyys)\n\n  for m in range(start_month, end_month + 1):\n    ms = \"{:02d}\".format(m)\n    #print(\"ms=\" + ms)\n    \n    # Source data file path\n    path_src_file = path_root_data + \"year=\" + yyyys + \"/month=\" +  ms + \"/type=yellow/yellow_tripdata_\" + yyyys + \"-\" + ms + \".csv\"\n    print(\"path_src_filepath=\" + path_src_file)\n    \n    # Correct schema to use\n    schema = GetTaxiSchema_Yellow(yyyy, m)\n    \n    # Read file to dataframe\n    df_file = GetDataFrameFromCsvFile(schema, path_src_file, \",\")\n    # df_file.printSchema()\n    \n    # Get dataframe with conformed schema\n    df_conformed = GetSchemaHomogenizedDataframe_Yellow(df_file, yyyy, m)\n    # df_conformed.printSchema()\n    \n    # Order columns per the canonical column list/order\n    df_canonical = df_conformed[columns_rides_canonical]\n    # df_canonical.printSchema()\n    \n    # Append this year/month to the full rides dataframe we're also building\n    df_all_rides_canonical = df_all_rides_canonical.union(df_canonical)\n    \n    # Write this year/month dataframe out to Parquet\n    df_canonical.coalesce(num_of_parquet_files).write.parquet(path_root_parquet_trips_yellow + yyyys + \"/\" + ms + \"/\")"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["#### Load taxi rides - Green"],"metadata":{}},{"cell_type":"markdown","source":["##### Define source file schemas - Green\n\nThese vary by year. We have to define several schemas to fit the different source file layouts."],"metadata":{}},{"cell_type":"code","source":["# Schema for source data based on year and month\n\n# 2016H2, 2017, 2018\nschema_rides_green_16H2to18 = StructType([\n    StructField(\"VendorID\", IntegerType(), True),\n    StructField(\"lpep_pickup_datetime\", TimestampType(), True),\n    StructField(\"lpep_dropoff_datetime\", TimestampType(), True),\n    StructField(\"store_and_fwd_flag\", StringType(), True),\n    StructField(\"RatecodeID\", IntegerType(), True),\n    StructField(\"PULocationID\", IntegerType(), True),\n    StructField(\"DOLocationID\", IntegerType(), True),\n    StructField(\"passenger_count\", IntegerType(), True),\n    StructField(\"trip_distance\", DoubleType(), True),\n    StructField(\"fare_amount\", DoubleType(), True),\n    StructField(\"extra\", DoubleType(), True),\n    StructField(\"mta_tax\", DoubleType(), True),\n    StructField(\"tip_amount\", DoubleType(), True),\n    StructField(\"tolls_amount\", DoubleType(), True),\n    StructField(\"ehail_fee\", DoubleType(), True),\n    StructField(\"improvement_surcharge\", DoubleType(), True),\n    StructField(\"total_amount\", DoubleType(), True),\n    StructField(\"payment_type\", IntegerType(), True),\n    StructField(\"trip_type\", IntegerType(), True)\n])\n\n# 2015 and 2016H1\nschema_rides_green_15to16H1 = StructType([\n    StructField(\"VendorID\", IntegerType(), True),\n    StructField(\"lpep_pickup_datetime\", TimestampType(), True),\n    StructField(\"Lpep_dropoff_datetime\", TimestampType(), True),\n    StructField(\"Store_and_fwd_flag\", StringType(), True),\n    StructField(\"RateCodeID\", IntegerType(), True),\n    StructField(\"Pickup_longitude\", DoubleType(), True),\n    StructField(\"Pickup_latitude\", DoubleType(), True),\n    StructField(\"Dropoff_longitude\", DoubleType(), True),\n    StructField(\"Dropoff_latitude\", DoubleType(), True),\n    StructField(\"Passenger_count\", IntegerType(), True),\n    StructField(\"Trip_distance\", DoubleType(), True),\n    StructField(\"Fare_amount\", DoubleType(), True),\n    StructField(\"Extra\", DoubleType(), True),\n    StructField(\"MTA_tax\", DoubleType(), True),\n    StructField(\"Tip_amount\", DoubleType(), True),\n    StructField(\"Tolls_amount\", DoubleType(), True),\n    StructField(\"Ehail_fee\", DoubleType(), True),\n    StructField(\"improvement_surcharge\", DoubleType(), True),\n    StructField(\"Total_amount\", DoubleType(), True),\n    StructField(\"Payment_type\", IntegerType(), True),\n    StructField(\"Trip_type\", IntegerType(), True)\n])\n\n# 2013 though 2014\nschema_rides_green_13to14 = StructType([\n    StructField(\"VendorID\", IntegerType(), True),\n    StructField(\"lpep_pickup_datetime\", TimestampType(), True),\n    StructField(\"Lpep_dropoff_datetime\", TimestampType(), True),\n    StructField(\"Store_and_fwd_flag\", StringType(), True),\n    StructField(\"RateCodeID\", IntegerType(), True),\n    StructField(\"Pickup_longitude\", DoubleType(), True),\n    StructField(\"Pickup_latitude\", DoubleType(), True),\n    StructField(\"Dropoff_longitude\", DoubleType(), True),\n    StructField(\"Dropoff_latitude\", DoubleType(), True),\n    StructField(\"Passenger_count\", IntegerType(), True),\n    StructField(\"Trip_distance\", DoubleType(), True),\n    StructField(\"Fare_amount\", DoubleType(), True),\n    StructField(\"Extra\", DoubleType(), True),\n    StructField(\"MTA_tax\", DoubleType(), True),\n    StructField(\"Tip_amount\", DoubleType(), True),\n    StructField(\"Tolls_amount\", DoubleType(), True),\n    StructField(\"Ehail_fee\", DoubleType(), True),\n    StructField(\"Total_amount\", DoubleType(), True),\n    StructField(\"Payment_type\", IntegerType(), True),\n    StructField(\"Trip_type\", IntegerType(), True)\n])\n"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["##### Schema helper functions - Green"],"metadata":{}},{"cell_type":"code","source":["# Function to add columns to dataframe as required to homogenize schema\n# Input:  Dataframe, year and month\n# Output: Dataframe with homogenized schema \n\ndef GetSchemaHomogenizedDataframe_Green(source_df, trip_year, trip_month):\n  years13To14 = [2013, 2014]\n\n  if (trip_year >= 2017 or (trip_year == 2016 and trip_month > 6)):\n    df = source_df\\\n      .withColumn(\"trip_year\", source_df.lpep_pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.lpep_pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"green\"))\\\n      .withColumnRenamed(\"VendorID\", \"vendor_id\")\\\n      .withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\")\\\n      .withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\\\n      .withColumnRenamed(\"RatecodeID\", \"rate_code_id\")\\\n      .withColumnRenamed(\"PULocationID\", \"pickup_location_id\")\\\n      .withColumnRenamed(\"DOLocationID\", \"dropoff_location_id\")\\\n      .withColumn(\"pickup_longitude\", lit(\"\"))\\\n      .withColumn(\"pickup_latitude\", lit(\"\"))\\\n      .withColumn(\"dropoff_longitude\", lit(\"\"))\\\n      .withColumn(\"dropoff_latitude\", lit(\"\"))\n\n      # passenger_count\n      # trip_distance\n      # store_and_fwd_flag\n      # payment_type\n      # fare_amount\n      # extra\n      # mta_tax\n      # tip_amount\n      # tolls_amount\n      # ehail_fee\n      # improvement_surcharge\n      # total_amount\n      # trip_type\n  elif ((trip_year == 2016 and trip_month <= 6) or (trip_year == 2015)):\n    df = source_df\\\n      .withColumn(\"trip_year\", source_df.lpep_pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.lpep_pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"green\"))\\\n      .withColumnRenamed(\"VendorID\", \"vendor_id\")\\\n      .withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\")\\\n      .withColumnRenamed(\"Lpep_dropoff_datetime\", \"dropoff_datetime\")\\\n      .withColumnRenamed(\"Passenger_count\", \"passenger_count\")\\\n      .withColumnRenamed(\"Trip_distance\", \"trip_distance\")\\\n      .withColumnRenamed(\"RateCodeID\", \"rate_code_id\")\\\n      .withColumnRenamed(\"Store_and_fwd_flag\", \"store_and_fwd_flag\")\\\n      .withColumn(\"pickup_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"dropoff_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"temp_pickup_longitude\", source_df.Pickup_longitude.cast(StringType()))\\\n      .drop(\"Pickup_longitude\")\\\n      .withColumnRenamed(\"temp_pickup_longitude\", \"pickup_longitude\")\\\n      .withColumn(\"temp_pickup_latitude\", source_df.Pickup_latitude.cast(StringType()))\\\n      .drop(\"Pickup_latitude\")\\\n      .withColumnRenamed(\"temp_pickup_latitude\", \"pickup_latitude\")\\\n      .withColumn(\"temp_dropoff_longitude\", source_df.Dropoff_longitude.cast(StringType()))\\\n      .drop(\"Dropoff_longitude\")\\\n      .withColumnRenamed(\"temp_dropoff_longitude\", \"dropoff_longitude\")\\\n      .withColumn(\"temp_dropoff_latitude\", source_df.Dropoff_latitude.cast(StringType()))\\\n      .drop(\"Dropoff_latitude\")\\\n      .withColumnRenamed(\"temp_dropoff_latitude\", \"dropoff_latitude\")\\\n      .withColumnRenamed(\"Payment_type\", \"payment_type\")\\\n      .withColumnRenamed(\"Fare_amount\", \"fare_amount\")\\\n      .withColumnRenamed(\"Extra\", \"extra\")\\\n      .withColumnRenamed(\"MTA_tax\", \"mta_tax\")\\\n      .withColumnRenamed(\"Tip_amount\", \"tip_amount\")\\\n      .withColumnRenamed(\"Tolls_amount\", \"tolls_amount\")\\\n      .withColumnRenamed(\"Ehail_fee\", \"ehail_fee\")\\\n      .withColumnRenamed(\"improvement_surcharge\", \"improvement_surcharge\")\\\n      .withColumnRenamed(\"Total_amount\", \"total_amount\")\\\n      .withColumnRenamed(\"Trip_type\", \"trip_type\")\n  elif (trip_year in years13To14):\n    df = source_df\\\n      .withColumn(\"trip_year\", source_df.lpep_pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.lpep_pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"green\"))\\\n      .withColumnRenamed(\"VendorID\", \"vendor_id\")\\\n      .withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\")\\\n      .withColumnRenamed(\"Lpep_dropoff_datetime\", \"dropoff_datetime\")\\\n      .withColumnRenamed(\"Passenger_count\", \"passenger_count\")\\\n      .withColumnRenamed(\"Trip_distance\", \"trip_distance\")\\\n      .withColumnRenamed(\"RateCodeID\", \"rate_code_id\")\\\n      .withColumnRenamed(\"Store_and_fwd_flag\", \"store_and_fwd_flag\")\\\n      .withColumn(\"pickup_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"dropoff_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"temp_pickup_longitude\", source_df.Pickup_longitude.cast(StringType()))\\\n      .drop(\"Pickup_longitude\")\\\n      .withColumnRenamed(\"temp_pickup_longitude\", \"pickup_longitude\")\\\n      .withColumn(\"temp_pickup_latitude\", source_df.Pickup_latitude.cast(StringType()))\\\n      .drop(\"Pickup_latitude\")\\\n      .withColumnRenamed(\"temp_pickup_latitude\", \"pickup_latitude\")\\\n      .withColumn(\"temp_dropoff_longitude\", source_df.Dropoff_longitude.cast(StringType()))\\\n      .drop(\"Dropoff_longitude\")\\\n      .withColumnRenamed(\"temp_dropoff_longitude\", \"dropoff_longitude\")\\\n      .withColumn(\"temp_dropoff_latitude\", source_df.Dropoff_latitude.cast(StringType()))\\\n      .drop(\"Dropoff_latitude\")\\\n      .withColumnRenamed(\"temp_dropoff_latitude\", \"dropoff_latitude\")\\\n\t  .withColumnRenamed(\"Payment_type\", \"payment_type\")\\\n      .withColumnRenamed(\"Fare_amount\", \"fare_amount\")\\\n      .withColumnRenamed(\"Extra\", \"extra\")\\\n      .withColumnRenamed(\"MTA_tax\", \"mta_tax\")\\\n      .withColumnRenamed(\"Tip_amount\", \"tip_amount\")\\\n      .withColumnRenamed(\"Tolls_amount\", \"tolls_amount\")\\\n      .withColumnRenamed(\"Ehail_fee\", \"ehail_fee\")\\\n      .withColumn(\"improvement_surcharge\",lit(0).cast(DoubleType()))\\\n      .withColumnRenamed(\"Total_amount\", \"total_amount\")\\\n      .withColumnRenamed(\"Trip_type\", \"trip_type\")\n  \n  return df;"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["# Function to return schema for a given year and month\n# Input:  Year and month\n# Output: StructType for applicable schema \n\ndef GetTaxiSchema_Green(trip_year, trip_month):\n  years13To14 = [2013, 2014]\n  \n  if (trip_year >= 2017 or (trip_year == 2016 and trip_month > 6)):\n    schema = schema_rides_green_16H2to18\n  elif ((trip_year == 2016 and trip_month <= 6) or trip_year == 2015):\n    schema = schema_rides_green_15to16H1\n  elif (trip_year in years13To14):\n    schema = schema_rides_green_13to14\n\n  return schema;"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["##### Load data files\n\n1. Load each year/month data file to a dataframe\n2. Persist to Parquet for that year/month\n3. Append to the \"all rides\" DataFrame"],"metadata":{}},{"cell_type":"code","source":["for yyyy in range(2013, 2019):\n  start_month = 1\n  end_month = 12\n\n  # The green dataset starts in August 2013 and goes up to June 2018 inclusive\n  # (at the time of this writing - if the dataset is expanded with all of 2018, this should be changed)\n  if yyyy == 2013:\n    start_month = 8\n  elif yyyy == 2018:\n    end_month = 6\n\n  #print(\"yyyy=\" + str(yyyy))\n  #print(\"start_month=\" + str(start_month))\n  #print(\"end_month=\" + str(end_month))\n\n  yyyys = str(yyyy)\n  #print(\"yyyys=\" + yyyys)\n\n  for m in range(start_month, end_month + 1):\n    ms = \"{:02d}\".format(m)\n    #print(\"ms=\" + ms)\n    \n    # Source data file path\n    path_src_file = path_root_data + \"year=\" + yyyys + \"/month=\" +  ms + \"/type=green/green_tripdata_\" + yyyys + \"-\" + ms + \".csv\"\n    print(\"path_src_filepath=\" + path_src_file)\n    \n    # Correct schema to use\n    schema = GetTaxiSchema_Green(yyyy, m)\n    \n    # Read file to dataframe\n    df_file = GetDataFrameFromCsvFile(schema, path_src_file, \",\")\n    # df_file.printSchema()\n    \n    # Get dataframe with conformed schema\n    df_conformed = GetSchemaHomogenizedDataframe_Green(df_file, yyyy, m)\n    # df_conformed.printSchema()\n    \n    # Order columns per the canonical column list/order\n    df_canonical = df_conformed[columns_rides_canonical]\n    # df_canonical.printSchema()\n    \n    # Append this year/month to the full rides dataframe we're also building\n    df_all_rides_canonical = df_all_rides_canonical.union(df_canonical)\n    \n    # Write this year/month dataframe out to Parquet\n    df_canonical.coalesce(num_of_parquet_files).write.parquet(path_root_parquet_trips_green + yyyys + \"/\" + ms + \"/\")"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["df_all_rides_canonical.coalesce(64).write.parquet(path_root_parquet_trips_all)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["## Delete Spark job files recursively\n\nCleanupSparkJobFiles(path_root_parquet)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["#### Discovery Day Lab - DONE\nAt this point, you are done with the minimum needed to proceed with Azure Discovery Day 2019.\nYou have ingested, transformed, and emitted data sufficient for later labs/tasks.\n\nIf you like, you can proceed with the optional steps below to explore additional capabilities on Spark. However, the following steps are not required for Discovery Day."],"metadata":{}},{"cell_type":"markdown","source":["Optional steps coming soon"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":59}],"metadata":{"name":"lab1","notebookId":3103873265031220},"nbformat":4,"nbformat_minor":0}
