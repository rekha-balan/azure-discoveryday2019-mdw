{"cells":[{"cell_type":"markdown","source":["# Azure Discovery Day 2019\n## Analytics with NRT Intelligence on Azure\n## Ingest, Transform, Emit\n\n#### Summary\nIn this Python Jupyter notebook, you will:\n1. Connect to Azure storage\n2. Ingest data from CSV files in Azure storage to Spark dataframes\n3. Conform and merge heterogenous data sets using the Spark dataframe API\n4. Emit data to Azure storage in Parquet file format\n\nAdditionally, there are optional steps to create Hive tables on the data, query them with Spark SQL, as well as some exploratory data analysis (EDA)."],"metadata":{}},{"cell_type":"code","source":["## Need some library includes\n\nimport os\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import broadcast, lit"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Variables"],"metadata":{}},{"cell_type":"code","source":["# Define some variables to minimize \"hard-coding\" in below cells. Note that variables could also be defined in a separate notebook for greater share-ability.\n\n# Azure storage account information. Note that in production, we would store sensitive info in an appropriate secret store,\n# such as a Databricks secret store backed by an Azure Key Vault. For a hackathon/workshop, we'll put them here for simplicity.\nstorage_acct_name = \"PROVIDE\"\nstorage_acct_key = \"PROVIDE\"\ncontainer_name = \"data\"\n\n# The mount point in the DBFS file system - this will look like a local folder but points to the Azure storage location\nmount_point = \"/mnt/\" + container_name + \"/\"\n\n# Reference data files path root\npath_root_ref = mount_point + \"reference-data/\"\n\n# Transactional data files path root\npath_root_data = mount_point + \"transactional-data-small/\"\n\n# Parquet data files output root\npath_root_parquet = mount_point + \"parquet/\"\n\n# Parquet ref data files output root\npath_root_parquet_ref = path_root_parquet + \"reference/\"\n\n# Parquet trip data files output root\npath_root_parquet_trips_yellow = path_root_parquet + \"trips-yellow/\"\npath_root_parquet_trips_green = path_root_parquet + \"trips-green/\"\npath_root_parquet_trips_all = path_root_parquet + \"trips-small/\"\n\n# Set number of Parquet files to output. Simply hard-code here, could also calculate based on ... # of worker nodes/cores etc.\nnum_of_parquet_files = 8"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Functions"],"metadata":{}},{"cell_type":"code","source":["# Function to get a Spark DataFrame from a CSV source file\n\ndef GetDataFrameFromCsvFile(schema, path_src_file, delimiter):\n  df = spark\\\n    .read\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"delimiter\", delimiter)\\\n    .schema(schema)\\\n    .load(path_src_file)\n  \n  return df;"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Given a reference dataframe (this would not make sense for large transaction dataframes), broadcast it across the cluster, lazy-cache it, and return the count, which instantiates the dataframe\n\ndef HandleReferenceDataFrame(df):\n  broadcast(df)\n  df.cache()\n  count = df.count()\n  \n  return count;"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Delete Spark job residual files (_SUCCESS, _start*, _committed*) down the folder/file hierarchy\n\ndef CleanupSparkJobFiles(path_root):\n  file_paths = GetFilesRecursive(path_root)\n  \n  for file_path in file_paths:\n    # Get just the file name\n    file_name = os.path.basename(file_path)\n    # print(file_name)\n    \n    if file_name.startswith(\"_\"):\n      # Temp job file - delete it\n      dbutils.fs.rm(file_path)\n    # elif file_name.endswith(\".parquet\"):\n      # Data file - no op\n    # else:\n      # Something else - no op"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Get iterable file list. Flattens hierarchical folder/file structure.\n\ndef GetFilesRecursive(path_root):\n  final_list = []\n\n  for directoryItem in dbutils.fs.ls(path_root):\n    directoryItemPathClean = directoryItem.path.replace(\"%25\", \"%\").replace(\"%25\", \"%\")\n    \n    if directoryItem.isDir() == True:\n      final_list = final_list + GetFilesRecursive(directoryItemPathClean)\n    else:\n      final_list.append(directoryItemPathClean)\n  \n  return final_list;"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Connect to Azure Storage"],"metadata":{}},{"cell_type":"code","source":["# Use the Databricks file system utilities to mount a Databricks file system location (/mnt/YOUR CONTAINER NAME) that points to the Azure storage account where data files are located\n# We use variables defined above and string concatenation here so that no \"hard-coding\" is needed\n# NOTE that this only needs to be done once as mounts survive cluster shutdown/restart.\n\ndbutils.fs.mount(\n  source = \"wasbs://\" + container_name + \"@\" + storage_acct_name + \".blob.core.windows.net\",\n  mount_point = mount_point,\n  extra_configs = {\"fs.azure.account.key.\" + storage_acct_name + \".blob.core.windows.net\":storage_acct_key}\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# This is included to remove the Azure storage mount\n# Commented out since not needed for the lab, but included here \"just in case\" for debugging/experimenting - for example, mount, unmount, try something different, mount again\n\n# dbutils.fs.unmount(mount_point)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# List contents of the Azure storage account to validate successful connect and mount\n# We are using the Databricks display() function here to improve the esthetics of the output\n\ndisplay(dbutils.fs.ls(mount_point))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Cleanup - delete Parquet output folder if it's present\n# NOTE!! This will DELETE any previous Parquet that you or others have emitted to this storage location!\n\ndbutils.fs.rm(path_root_parquet, True)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Load Reference Data Files into DataFrames"],"metadata":{}},{"cell_type":"markdown","source":["##### Define variables to hold the source path for each of the reference data files"],"metadata":{}},{"cell_type":"code","source":["src_file_ref_payment_type = path_root_ref + \"payment_type_lookup.csv\"\nsrc_file_ref_rate_code = path_root_ref + \"rate_code_lookup.csv\"\nsrc_file_ref_taxi_zone = path_root_ref + \"taxi_zone_lookup.csv\"\nsrc_file_ref_trip_month = path_root_ref + \"trip_month_lookup.csv\"\nsrc_file_ref_trip_type = path_root_ref + \"trip_type_lookup.csv\"\nsrc_file_ref_vendor = path_root_ref + \"vendor_lookup.csv\""],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["##### Define explicit schemas for each of the reference data files\n\nWe could also ingest files with schema inference (i.e. tell Spark to try to figure it out) but let's be explicit here for greater control."],"metadata":{}},{"cell_type":"code","source":["# Payment type\nschema_ref_payment_type = StructType([\n    StructField(\"payment_type\", IntegerType(), True),\n    StructField(\"abbreviation\", StringType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n# Rate code ID\nschema_ref_rate_code = StructType([\n    StructField(\"rate_code_id\", IntegerType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n# Taxi zone\nschema_ref_taxi_zone = StructType([\n    StructField(\"location_id\", StringType(), True),\n    StructField(\"borough\", StringType(), True),\n    StructField(\"zone\", StringType(), True),\n    StructField(\"service_zone\", StringType(), True)\n])\n\n# Trip month\nschema_ref_trip_month = StructType([\n    StructField(\"trip_month\", StringType(), True),\n    StructField(\"month_name_short\", StringType(), True),\n    StructField(\"month_name_full\", StringType(), True)\n])\n\n# Trip type\nschema_ref_trip_type = StructType([\n    StructField(\"trip_type\", IntegerType(), True),\n    StructField(\"description\", StringType(), True)\n])\n\n# Vendor ID\nschema_ref_vendor = StructType([\n    StructField(\"vendor_id\", IntegerType(), True),\n    StructField(\"abbreviation\", StringType(), True),\n    StructField(\"description\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["##### Load each reference data set into a Spark DataFrame\n\nWe load the data from source file into dataframe using a function (above) for that purpose.\n\nThen we do some more optimizations for the reference dataframes:\n1. Broadcast the dataframe. These are small dataframes with reference data. Broadcasting means we replicate a dataframe to each worker node in a Spark cluster, so that cross-node (cross-network) joins are avoided.\n2. Lazy-cache the dataframe into memory as another performance optimization.\n\nLast, we print the number rows in the dataframe."],"metadata":{}},{"cell_type":"code","source":["df_ref_payment_type = GetDataFrameFromCsvFile(schema_ref_payment_type, src_file_ref_payment_type, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_payment_type))\ndisplay(df_ref_payment_type)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["df_ref_rate_code = GetDataFrameFromCsvFile(schema_ref_rate_code, src_file_ref_rate_code, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_rate_code))\ndisplay(df_ref_rate_code)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["df_ref_taxi_zone = GetDataFrameFromCsvFile(schema_ref_taxi_zone, src_file_ref_taxi_zone, \",\")\n\nprint(HandleReferenceDataFrame(df_ref_taxi_zone))\ndisplay(df_ref_taxi_zone)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["df_ref_trip_month = GetDataFrameFromCsvFile(schema_ref_trip_month, src_file_ref_trip_month, \",\")\n\nprint(HandleReferenceDataFrame(df_ref_trip_month))\ndisplay(df_ref_trip_month)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["df_ref_trip_type = GetDataFrameFromCsvFile(schema_ref_trip_type, src_file_ref_trip_type, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_trip_type))\ndisplay(df_ref_trip_type)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["df_ref_vendor = GetDataFrameFromCsvFile(schema_ref_vendor, src_file_ref_vendor, \"|\")\n\nprint(HandleReferenceDataFrame(df_ref_vendor))\ndisplay(df_ref_vendor)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### Write reference data out to Parquet files\n\nParquet files are faster to load than CSV. They also support partitioning, but for the small reference data files, we coalesce the dataframe to 1 piece and we do not partition."],"metadata":{}},{"cell_type":"code","source":["df_ref_payment_type.coalesce(1).write.parquet(path_root_parquet_ref + \"payment-type/\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["df_ref_rate_code.coalesce(1).write.parquet(path_root_parquet_ref + \"rate-code/\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["df_ref_taxi_zone.coalesce(1).write.parquet(path_root_parquet_ref + \"taxi-zone/\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["df_ref_trip_month.coalesce(1).write.parquet(path_root_parquet_ref + \"trip-month/\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["df_ref_trip_type.coalesce(1).write.parquet(path_root_parquet_ref + \"trip-type/\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["df_ref_vendor.coalesce(1).write.parquet(path_root_parquet_ref + \"vendor/\")"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["### Load transactions (trip data) into DataFrames"],"metadata":{}},{"cell_type":"code","source":["## Canonical ordered column list to homogenize schema - we will conform all ingested data sets to this schema\n\ncolumns_rides_canonical = [\n    \"trip_type\",\n\t\"trip_year\",\n\t\"trip_month\",\n\t\"taxi_type\",\n\t\"vendor_id\",\n\t\"pickup_datetime\",\n\t\"dropoff_datetime\",\n\t\"passenger_count\",\n\t\"trip_distance\",\n\t\"rate_code_id\",\n\t\"store_and_fwd_flag\",\n\t\"pickup_location_id\",\n\t\"dropoff_location_id\",\n\t\"pickup_longitude\",\n\t\"pickup_latitude\",\n\t\"dropoff_longitude\",\n\t\"dropoff_latitude\",\n\t\"payment_type\",\n\t\"fare_amount\",\n\t\"extra\",\n\t\"mta_tax\",\n\t\"tip_amount\",\n\t\"tolls_amount\",\n\t\"improvement_surcharge\",\n    \"ehail_fee\",\n\t\"total_amount\"\n]"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Define canonical schema so that we can create an empty DataFrame with the canonical schema into which we will merge all the invididual data file-ingested DataFrames\n\nschema_rides_canonical = StructType([\n    StructField(\"trip_type\", IntegerType(), True),\n    StructField(\"trip_year\", StringType(), True),\n    StructField(\"trip_month\", StringType(), True),\n    StructField(\"taxi_type\", StringType(), True),\n    StructField(\"vendor_id\", IntegerType(), True),\n    StructField(\"pickup_datetime\", TimestampType(), True),\n    StructField(\"dropoff_datetime\", TimestampType(), True),\n    StructField(\"passenger_count\", IntegerType(), True),\n    StructField(\"trip_distance\", DoubleType(), True),\n    StructField(\"rate_code_id\", IntegerType(), True),\n    StructField(\"store_and_fwd_flag\", StringType(), True),\n    StructField(\"pickup_location_id\", IntegerType(), True),\n    StructField(\"dropoff_location_id\", IntegerType(), True),\n    StructField(\"pickup_longitude\", StringType(), True),\n    StructField(\"pickup_latitude\", StringType(), True),\n    StructField(\"dropoff_longitude\", StringType(), True),\n    StructField(\"dropoff_latitude\", StringType(), True),\n    StructField(\"payment_type\", IntegerType(), True),\n    StructField(\"fare_amount\", DoubleType(), True),\n    StructField(\"extra\", DoubleType(), True),\n    StructField(\"mta_tax\", DoubleType(), True),\n    StructField(\"tip_amount\", DoubleType(), True),\n    StructField(\"tolls_amount\", DoubleType(), True),\n    StructField(\"improvement_surcharge\", DoubleType(), True),\n    StructField(\"ehail_fee\", DoubleType(), True),\n    StructField(\"total_amount\", DoubleType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["#### Define a DataFrame to hold ALL rides - all years/months, both Yellow and Green"],"metadata":{}},{"cell_type":"code","source":["df_all_rides_canonical = spark.createDataFrame([], schema_rides_canonical)\ndf_all_rides_canonical.cache()\n\ndf_all_rides_canonical.printSchema()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["#### Load taxi rides - Yellow"],"metadata":{}},{"cell_type":"markdown","source":["##### Define source file schemas - Yellow\n\nThese vary by year. We have to define several schemas to fit the different source file layouts."],"metadata":{}},{"cell_type":"code","source":["## 2016H2, 2017, 2018\nschema_rides_yellow_16H2to18 = StructType([\n    StructField(\"VendorID\", IntegerType(), True),\n    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n    StructField(\"passenger_count\", IntegerType(), True),\n    StructField(\"trip_distance\", DoubleType(), True),\n    StructField(\"RatecodeID\", IntegerType(), True),\n    StructField(\"store_and_fwd_flag\", StringType(), True),\n    StructField(\"PULocationID\", IntegerType(), True),\n    StructField(\"DOLocationID\", IntegerType(), True),\n    StructField(\"payment_type\", IntegerType(), True),\n    StructField(\"fare_amount\", DoubleType(), True),\n    StructField(\"extra\", DoubleType(), True),\n    StructField(\"mta_tax\", DoubleType(), True),\n    StructField(\"tip_amount\", DoubleType(), True),\n    StructField(\"tolls_amount\", DoubleType(), True),\n    StructField(\"improvement_surcharge\", DoubleType(), True),\n    StructField(\"total_amount\", DoubleType(), True)\n])\n\n## 2015 and 2016H1\nschema_rides_yellow_15to16H1 = StructType([\n    StructField(\"VendorID\", IntegerType(), True),\n    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n    StructField(\"passenger_count\", IntegerType(), True),\n    StructField(\"trip_distance\", DoubleType(), True),\n    StructField(\"pickup_longitude\", DoubleType(), True),\n    StructField(\"pickup_latitude\", DoubleType(), True),\n    StructField(\"RatecodeID\", IntegerType(), True),\n    StructField(\"store_and_fwd_flag\", StringType(), True),\n    StructField(\"dropoff_longitude\", DoubleType(), True),\n    StructField(\"dropoff_latitude\", DoubleType(), True),\n    StructField(\"payment_type\", IntegerType(), True),\n    StructField(\"fare_amount\", DoubleType(), True),\n    StructField(\"extra\", DoubleType(), True),\n    StructField(\"mta_tax\", DoubleType(), True),\n    StructField(\"tip_amount\", DoubleType(), True),\n    StructField(\"tolls_amount\", DoubleType(), True),\n    StructField(\"improvement_surcharge\", DoubleType(), True),\n    StructField(\"total_amount\", DoubleType(), True)\n])\n\n## 2010 though 2014\nschema_rides_yellow_10to14 = StructType([\n    StructField(\"vendor_id\", StringType(), True),\n    StructField(\"pickup_datetime\", TimestampType(), True),\n    StructField(\"dropoff_datetime\", TimestampType(), True),\n    StructField(\"passenger_count\", IntegerType(), True),\n    StructField(\"trip_distance\", DoubleType(), True),\n    StructField(\"pickup_longitude\", DoubleType(), True),\n    StructField(\"pickup_latitude\", DoubleType(), True),\n    StructField(\"rate_code\", IntegerType(), True),\n    StructField(\"store_and_fwd_flag\", StringType(), True),\n    StructField(\"dropoff_longitude\", DoubleType(), True),\n    StructField(\"dropoff_latitude\", DoubleType(), True),\n    StructField(\"payment_type\", StringType(), True),\n    StructField(\"fare_amount\", DoubleType(), True),\n    StructField(\"surcharge\", DoubleType(), True),\n    StructField(\"mta_tax\", DoubleType(), True),\n    StructField(\"tip_amount\", DoubleType(), True),\n    StructField(\"tolls_amount\", DoubleType(), True),\n    StructField(\"total_amount\", DoubleType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["##### Schema helper functions - Yellow"],"metadata":{}},{"cell_type":"code","source":["# Function to add columns to dataframe as required to homogenize schema\n# Input:  Dataframe, year and month\n# Output: Dataframe with homogenized schema \n\ndef GetSchemaHomogenizedDataframe_Yellow(source_df, trip_year, trip_month):\n  years10To14 = [2010, 2011, 2012, 2013, 2014]\n  \n  if (trip_year >= 2017 or (trip_year == 2016 and trip_month > 6)):\n    df = source_df\\\n      .withColumn(\"trip_type\", lit(0))\\\n      .withColumn(\"trip_year\", source_df.tpep_pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.tpep_pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"yellow\"))\\\n      .withColumnRenamed(\"VendorID\", \"vendor_id\")\\\n      .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\\\n      .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\\\n      .withColumnRenamed(\"RatecodeID\", \"rate_code_id\")\\\n      .withColumnRenamed(\"PULocationID\", \"pickup_location_id\")\\\n      .withColumnRenamed(\"DOLocationID\", \"dropoff_location_id\")\\\n      .withColumn(\"pickup_longitude\", lit(\"\"))\\\n      .withColumn(\"pickup_latitude\", lit(\"\"))\\\n      .withColumn(\"dropoff_longitude\", lit(\"\"))\\\n      .withColumn(\"dropoff_latitude\", lit(\"\"))\\\n      .withColumn(\"ehail_fee\", lit(0.0))\n\n      # .withColumn(\"temp_payment_type\", source_df.payment_type.cast(StringType()))\\\n      # .drop(\"payment_type\")\\\n      # .withColumnRenamed(\"temp_payment_type\", \"payment_type\")\\\n\n      # passenger_count\n      # trip_distance\n      # store_and_fwd_flag\n      # fare_amount\n      # extra\n      # mta_tax\n      # tip_amount\n      # tolls_amount\n      # improvement_surcharge\n      # total_amount\n  elif ((trip_year == 2016 and trip_month <= 6) or (trip_year == 2015)):\n    df = source_df\\\n      .withColumn(\"trip_type\", lit(0))\\\n      .withColumn(\"trip_year\", source_df.tpep_pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.tpep_pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"yellow\"))\\\n      .withColumnRenamed(\"VendorID\", \"vendor_id\")\\\n      .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\\\n      .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\\\n      .withColumnRenamed(\"RatecodeID\", \"rate_code_id\")\\\n      .withColumn(\"pickup_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"dropoff_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"temp_pickup_longitude\", source_df.pickup_longitude.cast(StringType()))\\\n      .drop(\"pickup_longitude\")\\\n      .withColumnRenamed(\"temp_pickup_longitude\", \"pickup_longitude\")\\\n      .withColumn(\"temp_pickup_latitude\", source_df.pickup_latitude.cast(StringType()))\\\n      .drop(\"pickup_latitude\")\\\n      .withColumnRenamed(\"temp_pickup_latitude\", \"pickup_latitude\")\\\n      .withColumn(\"temp_dropoff_longitude\", source_df.dropoff_longitude.cast(StringType()))\\\n      .drop(\"dropoff_longitude\")\\\n      .withColumnRenamed(\"temp_dropoff_longitude\", \"dropoff_longitude\")\\\n      .withColumn(\"temp_dropoff_latitude\", source_df.dropoff_latitude.cast(StringType()))\\\n      .drop(\"dropoff_latitude\")\\\n      .withColumnRenamed(\"temp_dropoff_latitude\", \"dropoff_latitude\")\\\n      .withColumn(\"ehail_fee\", lit(0.0))\n\n      # .withColumn(\"temp_payment_type\", source_df.payment_type.cast(StringType()))\\\n      # .drop(\"payment_type\")\\\n      # .withColumnRenamed(\"temp_payment_type\", \"payment_type\")\\\n\n      # passenger_count\n      # trip_distance\n      # store_and_fwd_flag\n      # fare_amount\n      # extra\n      # mta_tax\n      # tip_amount\n      # tolls_amount\n      # improvement_surcharge\n      # total_amount\n  elif (trip_year in years10To14):\n    df = source_df\\\n      .withColumn(\"trip_type\", lit(0))\\\n      .withColumn(\"trip_year\", source_df.pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"yellow\"))\\\n      .withColumnRenamed(\"rate_code\", \"rate_code_id\")\\\n      .withColumn(\"pickup_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"dropoff_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"temp_pickup_longitude\", source_df.pickup_longitude.cast(StringType()))\\\n      .drop(\"pickup_longitude\")\\\n      .withColumnRenamed(\"temp_pickup_longitude\", \"pickup_longitude\")\\\n      .withColumn(\"temp_pickup_latitude\", source_df.pickup_latitude.cast(StringType()))\\\n      .drop(\"pickup_latitude\")\\\n      .withColumnRenamed(\"temp_pickup_latitude\", \"pickup_latitude\")\\\n      .withColumn(\"temp_dropoff_longitude\", source_df.dropoff_longitude.cast(StringType()))\\\n      .drop(\"dropoff_longitude\")\\\n      .withColumnRenamed(\"temp_dropoff_longitude\", \"dropoff_longitude\")\\\n      .withColumn(\"temp_dropoff_latitude\", source_df.dropoff_latitude.cast(StringType()))\\\n      .drop(\"dropoff_latitude\")\\\n      .withColumnRenamed(\"temp_dropoff_latitude\", \"dropoff_latitude\")\\\n      .withColumn(\"temp_payment_type\", source_df.payment_type.cast(StringType()))\\\n      .drop(\"payment_type\")\\\n      .withColumnRenamed(\"temp_payment_type\", \"payment_type\")\\\n      .withColumnRenamed(\"surcharge\", \"extra\")\\\n      .withColumn(\"improvement_surcharge\",lit(0).cast(DoubleType()))\\\n      .withColumn(\"ehail_fee\", lit(0.0))\n\n      # pickup_datetime\n      # dropoff_datetime\n      # passenger_count\n      # trip_distance \n      # store_and_fwd_flag\n      # payment_type\n      # fare_amount\n      # mta_tax\n      # tip_amount\n      # tolls_amount\n      # total_amount\n\n    # Yellow taxi data 2010-2014 has some ID columns that are actually the abbreviations. We need to replace those with the integer IDs.\n    # Vendor ID\n    df = df\\\n      .withColumnRenamed(\"vendor_id\", \"abbreviation\")\\\n      .join(df_ref_vendor, \"abbreviation\", \"outer\")\\\n      .drop(\"abbreviation\")\\\n      .drop(\"description\")\n\n    # Payment Type\n    df = df\\\n      .withColumnRenamed(\"payment_type\", \"abbreviation\")\\\n      .join(df_ref_payment_type, \"abbreviation\", \"outer\")\\\n      .drop(\"abbreviation\")\\\n      .drop(\"description\")\n\n  return df;"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# Function to return schema for a given year and month\n# Input:  Year and month\n# Output: StructType for applicable schema \n\ndef GetTaxiSchema_Yellow(trip_year, trip_month):\n  years10To14 = [2010, 2011, 2012, 2013, 2014]\n  \n  if (trip_year >= 2017 or (trip_year == 2016 and trip_month > 6)):\n    schema = schema_rides_yellow_16H2to18\n  elif ((trip_year == 2016 and trip_month <= 6) or trip_year == 2015):\n    schema = schema_rides_yellow_15to16H1\n  elif (trip_year in years10To14):\n    schema = schema_rides_yellow_10to14\n\n  return schema;"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["##### Load data files\n\n1. Load each year/month data file to a dataframe\n2. Persist to Parquet for that year/month\n3. Append to the \"all rides\" DataFrame"],"metadata":{}},{"cell_type":"markdown","source":["## NOTE for Azure Discovery Day Session\n\nIn the next cell (and the similar cell where we load Green taxi data) we are only using a SUBSET of years.\nWe are using 2017+. This is to generate smaller output files for you to work with in lab 2,\nso that lab 2 finishes in a timeframe that is reasonable for a one-day event like this.\n\nWe encourage you to try this with all years' data after today, if you are curious, by simply adjusting the start year."],"metadata":{}},{"cell_type":"code","source":["start_year_yellow = 2017\nend_year_yellow = 2019 # Remember this is Python so a for loop is an open-ended interval - this really means through 2018\n\nfor yyyy in range(start_year_yellow, end_year_yellow):\n  start_month = 1\n  end_month = 12\n\n  # The dataset goes up to June 2018 inclusive (at the time of this writing - if the dataset is expanded with all of 2018, this should be changed)\n  if yyyy == 2018:\n    end_month = 6\n\n  #print(\"yyyy=\" + str(yyyy))\n  #print(\"start_month=\" + str(start_month))\n  #print(\"end_month=\" + str(end_month))\n\n  yyyys = str(yyyy)\n  #print(\"yyyys=\" + yyyys)\n\n  for m in range(start_month, end_month + 1):\n    ms = \"{:02d}\".format(m)\n    #print(\"ms=\" + ms)\n    \n    # Source data file path\n    path_src_file = path_root_data + \"year=\" + yyyys + \"/month=\" +  ms + \"/type=yellow/yellow_tripdata_\" + yyyys + \"-\" + ms + \".csv\"\n    print(\"path_src_filepath=\" + path_src_file)\n    \n    # Correct schema to use\n    schema = GetTaxiSchema_Yellow(yyyy, m)\n    \n    # Read file to dataframe\n    df_file = GetDataFrameFromCsvFile(schema, path_src_file, \",\")\n    # df_file.printSchema()\n    \n    # Get dataframe with conformed schema\n    df_conformed = GetSchemaHomogenizedDataframe_Yellow(df_file, yyyy, m)\n    # df_conformed.printSchema()\n    \n    # Order columns per the canonical column list/order\n    df_canonical = df_conformed[columns_rides_canonical]\n    df_canonical.cache()\n    # df_canonical.printSchema()\n    \n    # If desired - write this year/month dataframe out to Parquet. This would not be required for the labs, but uncomment to experiment.\n    # df_canonical.coalesce(num_of_parquet_files).write.parquet(path_root_parquet_trips_yellow + yyyys + \"/\" + ms + \"/\")\n\n    # Append this year/month to the full rides dataframe we're building\n    df_all_rides_canonical = df_all_rides_canonical.union(df_canonical)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["#### Load taxi rides - Green"],"metadata":{}},{"cell_type":"markdown","source":["##### Define source file schemas - Green\n\nThese vary by year. We have to define several schemas to fit the different source file layouts."],"metadata":{}},{"cell_type":"code","source":["# Schema for source data based on year and month\n\n# 2016H2, 2017, 2018\nschema_rides_green_16H2to18 = StructType([\n    StructField(\"VendorID\", IntegerType(), True),\n    StructField(\"lpep_pickup_datetime\", TimestampType(), True),\n    StructField(\"lpep_dropoff_datetime\", TimestampType(), True),\n    StructField(\"store_and_fwd_flag\", StringType(), True),\n    StructField(\"RatecodeID\", IntegerType(), True),\n    StructField(\"PULocationID\", IntegerType(), True),\n    StructField(\"DOLocationID\", IntegerType(), True),\n    StructField(\"passenger_count\", IntegerType(), True),\n    StructField(\"trip_distance\", DoubleType(), True),\n    StructField(\"fare_amount\", DoubleType(), True),\n    StructField(\"extra\", DoubleType(), True),\n    StructField(\"mta_tax\", DoubleType(), True),\n    StructField(\"tip_amount\", DoubleType(), True),\n    StructField(\"tolls_amount\", DoubleType(), True),\n    StructField(\"ehail_fee\", DoubleType(), True),\n    StructField(\"improvement_surcharge\", DoubleType(), True),\n    StructField(\"total_amount\", DoubleType(), True),\n    StructField(\"payment_type\", IntegerType(), True),\n    StructField(\"trip_type\", IntegerType(), True)\n])\n\n# 2015 and 2016H1\nschema_rides_green_15to16H1 = StructType([\n    StructField(\"VendorID\", IntegerType(), True),\n    StructField(\"lpep_pickup_datetime\", TimestampType(), True),\n    StructField(\"Lpep_dropoff_datetime\", TimestampType(), True),\n    StructField(\"Store_and_fwd_flag\", StringType(), True),\n    StructField(\"RateCodeID\", IntegerType(), True),\n    StructField(\"Pickup_longitude\", DoubleType(), True),\n    StructField(\"Pickup_latitude\", DoubleType(), True),\n    StructField(\"Dropoff_longitude\", DoubleType(), True),\n    StructField(\"Dropoff_latitude\", DoubleType(), True),\n    StructField(\"Passenger_count\", IntegerType(), True),\n    StructField(\"Trip_distance\", DoubleType(), True),\n    StructField(\"Fare_amount\", DoubleType(), True),\n    StructField(\"Extra\", DoubleType(), True),\n    StructField(\"MTA_tax\", DoubleType(), True),\n    StructField(\"Tip_amount\", DoubleType(), True),\n    StructField(\"Tolls_amount\", DoubleType(), True),\n    StructField(\"Ehail_fee\", DoubleType(), True),\n    StructField(\"improvement_surcharge\", DoubleType(), True),\n    StructField(\"Total_amount\", DoubleType(), True),\n    StructField(\"Payment_type\", IntegerType(), True),\n    StructField(\"Trip_type\", IntegerType(), True)\n])\n\n# 2013 though 2014\nschema_rides_green_13to14 = StructType([\n    StructField(\"VendorID\", IntegerType(), True),\n    StructField(\"lpep_pickup_datetime\", TimestampType(), True),\n    StructField(\"Lpep_dropoff_datetime\", TimestampType(), True),\n    StructField(\"Store_and_fwd_flag\", StringType(), True),\n    StructField(\"RateCodeID\", IntegerType(), True),\n    StructField(\"Pickup_longitude\", DoubleType(), True),\n    StructField(\"Pickup_latitude\", DoubleType(), True),\n    StructField(\"Dropoff_longitude\", DoubleType(), True),\n    StructField(\"Dropoff_latitude\", DoubleType(), True),\n    StructField(\"Passenger_count\", IntegerType(), True),\n    StructField(\"Trip_distance\", DoubleType(), True),\n    StructField(\"Fare_amount\", DoubleType(), True),\n    StructField(\"Extra\", DoubleType(), True),\n    StructField(\"MTA_tax\", DoubleType(), True),\n    StructField(\"Tip_amount\", DoubleType(), True),\n    StructField(\"Tolls_amount\", DoubleType(), True),\n    StructField(\"Ehail_fee\", DoubleType(), True),\n    StructField(\"Total_amount\", DoubleType(), True),\n    StructField(\"Payment_type\", IntegerType(), True),\n    StructField(\"Trip_type\", IntegerType(), True)\n])\n"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["##### Schema helper functions - Green"],"metadata":{}},{"cell_type":"code","source":["# Function to add columns to dataframe as required to homogenize schema\n# Input:  Dataframe, year and month\n# Output: Dataframe with homogenized schema \n\ndef GetSchemaHomogenizedDataframe_Green(source_df, trip_year, trip_month):\n  years13To14 = [2013, 2014]\n\n  if (trip_year >= 2017 or (trip_year == 2016 and trip_month > 6)):\n    df = source_df\\\n      .withColumn(\"trip_year\", source_df.lpep_pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.lpep_pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"green\"))\\\n      .withColumnRenamed(\"VendorID\", \"vendor_id\")\\\n      .withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\")\\\n      .withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\\\n      .withColumnRenamed(\"RatecodeID\", \"rate_code_id\")\\\n      .withColumnRenamed(\"PULocationID\", \"pickup_location_id\")\\\n      .withColumnRenamed(\"DOLocationID\", \"dropoff_location_id\")\\\n      .withColumn(\"pickup_longitude\", lit(\"\"))\\\n      .withColumn(\"pickup_latitude\", lit(\"\"))\\\n      .withColumn(\"dropoff_longitude\", lit(\"\"))\\\n      .withColumn(\"dropoff_latitude\", lit(\"\"))\n\n      # passenger_count\n      # trip_distance\n      # store_and_fwd_flag\n      # payment_type\n      # fare_amount\n      # extra\n      # mta_tax\n      # tip_amount\n      # tolls_amount\n      # ehail_fee\n      # improvement_surcharge\n      # total_amount\n      # trip_type\n  elif ((trip_year == 2016 and trip_month <= 6) or (trip_year == 2015)):\n    df = source_df\\\n      .withColumn(\"trip_year\", source_df.lpep_pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.lpep_pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"green\"))\\\n      .withColumnRenamed(\"VendorID\", \"vendor_id\")\\\n      .withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\")\\\n      .withColumnRenamed(\"Lpep_dropoff_datetime\", \"dropoff_datetime\")\\\n      .withColumnRenamed(\"Passenger_count\", \"passenger_count\")\\\n      .withColumnRenamed(\"Trip_distance\", \"trip_distance\")\\\n      .withColumnRenamed(\"RateCodeID\", \"rate_code_id\")\\\n      .withColumnRenamed(\"Store_and_fwd_flag\", \"store_and_fwd_flag\")\\\n      .withColumn(\"pickup_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"dropoff_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"temp_pickup_longitude\", source_df.Pickup_longitude.cast(StringType()))\\\n      .drop(\"Pickup_longitude\")\\\n      .withColumnRenamed(\"temp_pickup_longitude\", \"pickup_longitude\")\\\n      .withColumn(\"temp_pickup_latitude\", source_df.Pickup_latitude.cast(StringType()))\\\n      .drop(\"Pickup_latitude\")\\\n      .withColumnRenamed(\"temp_pickup_latitude\", \"pickup_latitude\")\\\n      .withColumn(\"temp_dropoff_longitude\", source_df.Dropoff_longitude.cast(StringType()))\\\n      .drop(\"Dropoff_longitude\")\\\n      .withColumnRenamed(\"temp_dropoff_longitude\", \"dropoff_longitude\")\\\n      .withColumn(\"temp_dropoff_latitude\", source_df.Dropoff_latitude.cast(StringType()))\\\n      .drop(\"Dropoff_latitude\")\\\n      .withColumnRenamed(\"temp_dropoff_latitude\", \"dropoff_latitude\")\\\n      .withColumnRenamed(\"Payment_type\", \"payment_type\")\\\n      .withColumnRenamed(\"Fare_amount\", \"fare_amount\")\\\n      .withColumnRenamed(\"Extra\", \"extra\")\\\n      .withColumnRenamed(\"MTA_tax\", \"mta_tax\")\\\n      .withColumnRenamed(\"Tip_amount\", \"tip_amount\")\\\n      .withColumnRenamed(\"Tolls_amount\", \"tolls_amount\")\\\n      .withColumnRenamed(\"Ehail_fee\", \"ehail_fee\")\\\n      .withColumnRenamed(\"improvement_surcharge\", \"improvement_surcharge\")\\\n      .withColumnRenamed(\"Total_amount\", \"total_amount\")\\\n      .withColumnRenamed(\"Trip_type\", \"trip_type\")\n  elif (trip_year in years13To14):\n    df = source_df\\\n      .withColumn(\"trip_year\", source_df.lpep_pickup_datetime[0:4])\\\n      .withColumn(\"trip_month\", source_df.lpep_pickup_datetime[6:2])\\\n      .withColumn(\"taxi_type\", lit(\"green\"))\\\n      .withColumnRenamed(\"VendorID\", \"vendor_id\")\\\n      .withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\")\\\n      .withColumnRenamed(\"Lpep_dropoff_datetime\", \"dropoff_datetime\")\\\n      .withColumnRenamed(\"Passenger_count\", \"passenger_count\")\\\n      .withColumnRenamed(\"Trip_distance\", \"trip_distance\")\\\n      .withColumnRenamed(\"RateCodeID\", \"rate_code_id\")\\\n      .withColumnRenamed(\"Store_and_fwd_flag\", \"store_and_fwd_flag\")\\\n      .withColumn(\"pickup_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"dropoff_location_id\", lit(0).cast(IntegerType()))\\\n      .withColumn(\"temp_pickup_longitude\", source_df.Pickup_longitude.cast(StringType()))\\\n      .drop(\"Pickup_longitude\")\\\n      .withColumnRenamed(\"temp_pickup_longitude\", \"pickup_longitude\")\\\n      .withColumn(\"temp_pickup_latitude\", source_df.Pickup_latitude.cast(StringType()))\\\n      .drop(\"Pickup_latitude\")\\\n      .withColumnRenamed(\"temp_pickup_latitude\", \"pickup_latitude\")\\\n      .withColumn(\"temp_dropoff_longitude\", source_df.Dropoff_longitude.cast(StringType()))\\\n      .drop(\"Dropoff_longitude\")\\\n      .withColumnRenamed(\"temp_dropoff_longitude\", \"dropoff_longitude\")\\\n      .withColumn(\"temp_dropoff_latitude\", source_df.Dropoff_latitude.cast(StringType()))\\\n      .drop(\"Dropoff_latitude\")\\\n      .withColumnRenamed(\"temp_dropoff_latitude\", \"dropoff_latitude\")\\\n\t  .withColumnRenamed(\"Payment_type\", \"payment_type\")\\\n      .withColumnRenamed(\"Fare_amount\", \"fare_amount\")\\\n      .withColumnRenamed(\"Extra\", \"extra\")\\\n      .withColumnRenamed(\"MTA_tax\", \"mta_tax\")\\\n      .withColumnRenamed(\"Tip_amount\", \"tip_amount\")\\\n      .withColumnRenamed(\"Tolls_amount\", \"tolls_amount\")\\\n      .withColumnRenamed(\"Ehail_fee\", \"ehail_fee\")\\\n      .withColumn(\"improvement_surcharge\",lit(0).cast(DoubleType()))\\\n      .withColumnRenamed(\"Total_amount\", \"total_amount\")\\\n      .withColumnRenamed(\"Trip_type\", \"trip_type\")\n  \n  return df;"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# Function to return schema for a given year and month\n# Input:  Year and month\n# Output: StructType for applicable schema \n\ndef GetTaxiSchema_Green(trip_year, trip_month):\n  years13To14 = [2013, 2014]\n  \n  if (trip_year >= 2017 or (trip_year == 2016 and trip_month > 6)):\n    schema = schema_rides_green_16H2to18\n  elif ((trip_year == 2016 and trip_month <= 6) or trip_year == 2015):\n    schema = schema_rides_green_15to16H1\n  elif (trip_year in years13To14):\n    schema = schema_rides_green_13to14\n\n  return schema;"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["##### Load data files\n\n1. Load each year/month data file to a dataframe\n2. Persist to Parquet for that year/month\n3. Append to the \"all rides\" DataFrame"],"metadata":{}},{"cell_type":"markdown","source":["## NOTE for Azure Discovery Day Session\n\nIn the next cell (and the similar cell where we load Yellow taxi data) we are only using a SUBSET of years.\nWe are using 2017+. This is to generate smaller output files for you to work with in lab 2,\nso that lab 2 finishes in a timeframe that is reasonable for a one-day event like this.\n\nWe encourage you to try this with all years' data after today, if you are curious, by simply adjusting the start year."],"metadata":{}},{"cell_type":"code","source":["start_year_green = 2017\nend_year_green = 2019 # Remember this is Python so a for loop is an open-ended interval - this really means through 2018\n\nfor yyyy in range(start_year_green, end_year_green):\n  start_month = 1\n  end_month = 12\n\n  # The green dataset starts in August 2013 and goes up to June 2018 inclusive\n  # (at the time of this writing - if the dataset is expanded with all of 2018, this should be changed)\n  if yyyy == 2013:\n    start_month = 8\n  elif yyyy == 2018:\n    end_month = 6\n\n  #print(\"yyyy=\" + str(yyyy))\n  #print(\"start_month=\" + str(start_month))\n  #print(\"end_month=\" + str(end_month))\n\n  yyyys = str(yyyy)\n  #print(\"yyyys=\" + yyyys)\n\n  for m in range(start_month, end_month + 1):\n    ms = \"{:02d}\".format(m)\n    #print(\"ms=\" + ms)\n    \n    # Source data file path\n    path_src_file = path_root_data + \"year=\" + yyyys + \"/month=\" +  ms + \"/type=green/green_tripdata_\" + yyyys + \"-\" + ms + \".csv\"\n    print(\"path_src_filepath=\" + path_src_file)\n    \n    # Correct schema to use\n    schema = GetTaxiSchema_Green(yyyy, m)\n    \n    # Read file to dataframe\n    df_file = GetDataFrameFromCsvFile(schema, path_src_file, \",\")\n    # df_file.printSchema()\n    \n    # Get dataframe with conformed schema\n    df_conformed = GetSchemaHomogenizedDataframe_Green(df_file, yyyy, m)\n    # df_conformed.printSchema()\n    \n    # Order columns per the canonical column list/order\n    df_canonical = df_conformed[columns_rides_canonical]\n    df_canonical.cache()\n    # df_canonical.printSchema()\n    \n    # If desired - write this year/month dataframe out to Parquet. This would not be required for the labs, but uncomment to experiment.\n    # df_canonical.coalesce(num_of_parquet_files).write.parquet(path_root_parquet_trips_green + yyyys + \"/\" + ms + \"/\")\n    \n    # Append this year/month to the full rides dataframe we're also building\n    df_all_rides_canonical = df_all_rides_canonical.union(df_canonical)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["# Write combined data out to Parquet\n\n# NOTE that we are not partitioning data here, for simplicity. However, partitioning may make sense based on\n# likely query patterns, if this data will be queried (e.g. from Hive SQL, see below) repeatedly and if queries are,\n# for example, likely to have predicates such as specific date ranges, in which case we might partition by trip_year and # trip_month.\n\ndf_all_rides_canonical.coalesce(8).write.parquet(path_root_parquet_trips_all)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["## Delete Spark job files recursively\n\nCleanupSparkJobFiles(path_root_parquet)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["# Discovery Day Lab - COMPLETE\n\nAt this point, you are done with the minimum needed to proceed with Azure Discovery Day 2019.\nYou have ingested, transformed, and emitted data sufficient for later labs/tasks.\n\nIf you like, you can proceed with the optional steps below to explore additional capabilities on Spark. However, the following steps are not required for Discovery Day."],"metadata":{}},{"cell_type":"markdown","source":["### Hive and Spark SQL"],"metadata":{}},{"cell_type":"markdown","source":["##### We can define Hive tables over the Parquet data we wrote to storage, then query those tables with SQL"],"metadata":{}},{"cell_type":"code","source":["db_name = \"discdaydb\""],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["# List current Hive databases\n\ndisplay(spark.catalog.listDatabases())"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["# Cleanup - drop the database first, if it already exists\n\nspark.sql(\"DROP DATABASE IF EXISTS \" + db_name + \" CASCADE\")"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["# Create the database\n\nspark.sql(\"CREATE DATABASE IF NOT EXISTS \" + db_name)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["# Set default database for further operations so we don't constantly have to specify this explicitly\n\nspark.catalog.setCurrentDatabase(db_name)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["# List tables in our database. There should not be any, since we just created this database.\n# If this is run without specifying a db name, it will use the current database set above\n\nspark.catalog.listTables(db_name)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["##### Reference data tables\n\nClean up first by dropping if exists, then create new tables on top of the Parquet folders we wrote to above.\nThen just get a count for each table's rows as a quick sanity check. Naturally you can write other SQL statements there."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS payment_type\")\nspark.sql(\"DROP TABLE IF EXISTS rate_code\")\nspark.sql(\"DROP TABLE IF EXISTS taxi_zone\")\nspark.sql(\"DROP TABLE IF EXISTS trip_month\")\nspark.sql(\"DROP TABLE IF EXISTS trip_type\")\nspark.sql(\"DROP TABLE IF EXISTS vendor\")"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["spark.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS payment_type(\npayment_type INT,\nabbreviation STRING,\ndescription STRING)\nUSING parquet\nLOCATION '\"\"\" + path_root_parquet_ref + \"\"\"payment-type/'\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["spark.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS rate_code(\nrate_code_id INT,\ndescription STRING)\nUSING parquet\nLOCATION '\"\"\" + path_root_parquet_ref + \"\"\"rate-code/'\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["spark.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS taxi_zone(\nlocation_id STRING,\nborough STRING,\nzone STRING,\nservice_zone STRING)\nUSING parquet\nLOCATION '\"\"\" + path_root_parquet_ref + \"\"\"taxi-zone/'\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["spark.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS trip_month(\ntrip_month STRING,\nmonth_name_short STRING,\nmonth_name_full STRING)\nUSING parquet\nLOCATION '\"\"\" + path_root_parquet_ref + \"\"\"trip-month/'\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["spark.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS trip_type(\ntrip_type INT,\ndescription STRING)\nUSING parquet\nLOCATION '\"\"\" + path_root_parquet_ref + \"\"\"trip-type/'\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["spark.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS vendor(\nvendor_id INT,\nabbreviation STRING,\ndescription STRING)\nUSING parquet\nLOCATION '\"\"\" + path_root_parquet_ref + \"\"\"vendor/'\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM payment_type\"))"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM rate_code\"))"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM taxi_zone\"))"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM trip_month\"))"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM trip_type\"))"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM vendor\"))"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":["##### Trip table\n\nWe will use the combined (schema-conformed, merged data for both companies) data"],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS trips_all\")"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["spark.sql(\"\"\"\n  CREATE TABLE trips_all(\n    trip_type INT,\n    trip_year STRING,\n    trip_month STRING,\n    taxi_type STRING,\n    vendor_id INT,\n    pickup_datetime TIMESTAMP,\n    dropoff_datetime TIMESTAMP,\n    passenger_count INT,\n    trip_distance DOUBLE,\n    rate_code_id INT,\n    store_and_fwd_flag STRING,\n    pickup_location_id INT,\n    dropoff_location_id INT,\n    pickup_longitude STRING,\n    pickup_latitude STRING,\n    dropoff_longitude STRING,\n    dropoff_latitude STRING,\n    payment_type INT,\n    fare_amount DOUBLE,\n    extra DOUBLE,\n    mta_tax DOUBLE,\n    tip_amount DOUBLE,\n    tolls_amount DOUBLE,\n    improvement_surcharge DOUBLE,\n    ehail_fee DOUBLE,\n    total_amount DOUBLE)\n  USING parquet\n  LOCATION '\"\"\" + path_root_parquet_trips_all + \"\"\"'\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["# 99880979\n\ndisplay(spark.sql(\"SELECT COUNT(*) FROM trips_all\"))"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":["## Data Exploration"],"metadata":{}},{"cell_type":"markdown","source":["##### Denormalized DataFrame\n\nHere, we join the trips table to each of the reference tables to get reference descriptions etc."],"metadata":{}},{"cell_type":"code","source":["df_full = spark.sql(\"\"\"\nselect  \n    t.trip_type,\n    t.trip_year,\n    t.trip_month,\n    t.taxi_type,\n    t.vendor_id,\n    t.pickup_datetime,\n    t.dropoff_datetime,\n    t.passenger_count,\n    t.trip_distance,\n    t.rate_code_id,\n    t.store_and_fwd_flag,\n    t.pickup_location_id,\n    t.dropoff_location_id,\n    t.pickup_longitude,\n    t.pickup_latitude,\n    t.dropoff_longitude,\n    t.dropoff_latitude,\n    t.payment_type,\n    t.fare_amount,\n    t.extra,\n    t.mta_tax,\n    t.tip_amount,\n    t.tolls_amount,\n    t.improvement_surcharge,\n    t.ehail_fee,\n    t.total_amount,\n    pt.description as payment_type_description,\n    rc.description as rate_code_description,\n    tzpu.borough as pickup_borough,\n    tzpu.zone as pickup_zone,\n    tzpu.service_zone as pickup_service_zone,\n    tzdo.borough as dropoff_borough,\n    tzdo.zone as dropoff_zone,\n    tzdo.service_zone as dropoff_service_zone,\n    tm.month_name_short,\n    tm.month_name_full,\n    tt.description as trip_type_description,\n    v.abbreviation as vendor_abbreviation,\n    v.description as vendor_description,\n    year(t.pickup_datetime) as pickup_year,\n    month(t.pickup_datetime) as pickup_month,\n    day(t.pickup_datetime) as pickup_day,\n    hour(t.pickup_datetime) as pickup_hour,\n    minute(t.pickup_datetime) as pickup_minute,\n    second(t.pickup_datetime) as pickup_second,\n    date(t.pickup_datetime) as pickup_date,\n    year(t.dropoff_datetime) as dropoff_year,\n    month(t.dropoff_datetime) as dropoff_month,\n    day(t.dropoff_datetime) as dropoff_day,\n    hour(t.dropoff_datetime) as dropoff_hour,\n    minute(t.dropoff_datetime) as dropoff_minute,\n    second(t.dropoff_datetime) as dropoff_second,\n    date(t.dropoff_datetime) as dropoff_date\nfrom\n    trips_all t\n    left outer join payment_type pt on (t.payment_type = pt.payment_type)\n    left outer join rate_code rc on (t.rate_code_id = rc.rate_code_id)\n    left outer join taxi_zone tzpu on (t.pickup_location_id = tzpu.location_id)\n    left outer join taxi_zone tzdo on (t.dropoff_location_id = tzdo.location_id)\n    left outer join trip_month tm on (t.trip_month = tm.trip_month)\n    left outer join trip_type tt on (t.trip_type = tt.trip_type)\n    left outer join vendor v on (t.vendor_id = v.vendor_id)\"\"\"\n)"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["# We cache the data frame. Given its size... we may run into memory pressure here. Check your cluster size/VM types.\n\ndf_full.cache()"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["# 99880979\n\nprint(df_full.count())"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["# Drop duplicate records\n\ndf_full = df_full.dropDuplicates()"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["# Count after dropping duplicates. This will take several minutes, as we have close to 100 million rows.\n# After dedupe, count is 99874067, which is less than the original (pre-join) dataframe.\n\nprint(df_full.count())"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["# Print the data frame schema\n\ndf_full.printSchema()"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["df_description = df_full.describe()\ndf_description.cache()"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["# Summary statistics\n# This uncovers some interesting issues in our data! Can you spot any right away?\n\ndisplay(df_description)"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["# Get top rows - head(n) or take(n)\n\ndisplay(df_full.head(25))"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["# Dataframe explanation\n\ndf_full.explain()"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["# Quantiles in a column\n\ndf_full.approxQuantile(\"total_amount\", [0.25, 0.5, 0.75], 0.1)"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["# Quantiles in a column\n\ndf_full.approxQuantile(\"trip_distance\", [0.25, 0.5, 0.75], 0.1)"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["# Frequent items\n\ndisplay(df_full.freqItems([\"pickup_hour\"]))"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["# Check correlation between two fields\n\ndf_full.corr(\"total_amount\", \"trip_distance\")"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":102}],"metadata":{"name":"lab1","notebookId":1874438378291896},"nbformat":4,"nbformat_minor":0}
